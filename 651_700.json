[
  {
    "id": 1651,
    "q": "A company wants to configure its Amazon CloudFront distribution to use SSL/TLS certificates. The company does not want to use the default domain name for the distribution. Instead, the company wants to use a different domain name for the distribution. Which solution will deploy the certificate without incurring any additional costs?",
    "o": [
      {
        "c": "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-east-1 Region.",
        "a": "no"
      },
      {
        "c": "Request an Amazon issued private certificate from AWS Certificate Manager (ACM) in the us-west-1 Region.",
        "a": "no"
      },
      {
        "c": "Request an Amazon issued public certificate from AWS Certificate Manager (ACM) in the us-east-1 Region.",
        "a": "yes"
      },
      {
        "c": "Request an Amazon issued public certificate from AWS Certificate Manager (ACM) in the us-west-1 Region.",
        "a": "no"
      }
    ],
    "nt": "AWS Certificate Manager (ACM) provides SSL/TLS certificates at no additional cost when used with supported AWS services like CloudFront. CloudFront requires certificates to be in the us-east-1 Region specifically. Public certificates from ACM are free, while private certificates incur costs. Therefore, requesting a public certificate from ACM in us-east-1 meets the requirement without additional costs."
  },
  {
    "id": 1652,
    "q": "A company creates operations data and stores the data in an Amazon S3 bucket. For the company's annual audit, an external consultant needs to access an annual report that is stored in the S3 bucket. The external consultant needs to access the report for 7 days. The company must implement a solution to allow the external consultant access to only the report. Which solution will meet these requirements with the MOST operational efficiency?",
    "o": [
      {
        "c": "Create a new S3 bucket that is configured to host a public static website. Migrate the operations data to the new S3 bucket. Share the S3 website URL with the external consultant.",
        "a": "no"
      },
      {
        "c": "Enable public access to the S3 bucket for 7 days. Remove access to the S3 bucket when the external consultant completes the audit.",
        "a": "no"
      },
      {
        "c": "Create a new IAM user that has access to the report in the S3 bucket. Provide the access keys to the external consultant. Revoke the access keys after 7 days.",
        "a": "no"
      },
      {
        "c": "Generate a presigned URL that has the required access to the location of the report on the S3 bucket. Share the presigned URL with the external consultant.",
        "a": "yes"
      }
    ],
    "nt": "Presigned URLs provide time-limited access to specific S3 objects without requiring the external user to have AWS credentials or IAM permissions. They are easy to generate and automatically expire after the specified duration, making them operationally efficient for temporary external access to specific files."
  },
  {
    "id": 1653,
    "q": "A company plans to run a high performance computing (HPC) workload on Amazon EC2 Instances. The workload requires low-latency network performance and high network throughput with tightly coupled node-to-node communication. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Configure the EC2 instances to be part of a cluster placement group.",
        "a": "yes"
      },
      {
        "c": "Launch the EC2 instances with Dedicated Instance tenancy.",
        "a": "no"
      },
      {
        "c": "Launch the EC2 instances as Spot Instances.",
        "a": "no"
      },
      {
        "c": "Configure an On-Demand Capacity Reservation when the EC2 instances are launched.",
        "a": "no"
      }
    ],
    "nt": "Cluster placement groups provide the lowest latency and highest throughput network performance by grouping instances within a single Availability Zone. This is essential for HPC workloads with tightly coupled node-to-node communication that requires minimal network latency."
  },
  {
    "id": 1654,
    "q": "A company has primary and secondary data centers that are 500 miles (804.7 km) apart and interconnected with high-speed fiber-optic cable. The company needs a highly available and secure network connection between its data centers and a VPC on AWS for a mission-critical workload. A solutions architect must choose a connection solution that provides maximum resiliency. Which solution meets these requirements?",
    "o": [
      {
        "c": "Two AWS Direct Connect connections from the primary data center terminating at two Direct Connect locations on two separate devices.",
        "a": "no"
      },
      {
        "c": "A single AWS Direct Connect connection from each of the primary and secondary data centers terminating at one Direct Connect location on the same device.",
        "a": "no"
      },
      {
        "c": "Two AWS Direct Connect connections from each of the primary and secondary data centers terminating at two Direct Connect locations on two separate devices.",
        "a": "yes"
      },
      {
        "c": "A single AWS Direct Connect connection from each of the primary and secondary data centers terminating at one Direct Connect location on two separate devices.",
        "a": "no"
      }
    ],
    "nt": "This configuration provides maximum resiliency through complete redundancy: multiple connections from both data centers, multiple Direct Connect locations, and multiple devices. This ensures that if any single component fails (connection, location, or device), the network connectivity remains available."
  },
  {
    "id": 1655,
    "q": "A company runs several Amazon RDS for Oracle On-Demand DB instances that have high utilization. The RDS DB instances run in member accounts that are in an organization in AWS Organizations. The company's finance team has access to the organization's management account and member accounts. The finance team wants to find ways to optimize costs by using AWS Trusted Advisor. Which combination of steps will meet these requirements? (Choose two.)",
    "o": [
      {
        "c": "Use the Trusted Advisor recommendations in the management account.",
        "a": "yes"
      },
      {
        "c": "Use the Trusted Advisor recommendations in the member accounts where the RDS DB instances are running.",
        "a": "no"
      },
      {
        "c": "Review the Trusted Advisor checks for Amazon RDS Reserved Instance Optimization.",
        "a": "yes"
      },
      {
        "c": "Review the Trusted Advisor checks for Amazon RDS Idle DB Instances.",
        "a": "no"
      },
      {
        "c": "Review the Trusted Advisor checks for compute optimization. Crosscheck the results by using AWS Compute Optimizer.",
        "a": "no"
      }
    ],
    "nt": "Trusted Advisor in the management account provides organization-wide cost optimization recommendations. The RDS Reserved Instance Optimization check specifically identifies opportunities to save costs by purchasing Reserved Instances for heavily utilized On-Demand DB instances."
  },
  {
    "id": 1656,
    "q": "A solutions architect is creating an application. The application will run on Amazon EC2 instances in private subnets across multiple Availability Zones in a VPC. The EC2 instances will frequently access large files that contain confidential information. These files are stored in Amazon S3 buckets for processing. The solutions architect must optimize the network architecture to minimize data transfer costs. What should the solutions architect do to meet these requirements?",
    "o": [
      {
        "c": "Create a gateway endpoint for Amazon S3 in the VPC. In the route tables for the private subnets, add an entry for the gateway endpoint.",
        "a": "yes"
      },
      {
        "c": "Create a single NAT gateway in a public subnet. In the route tables for the private subnets, add a default route that points to the NAT gateway.",
        "a": "no"
      },
      {
        "c": "Create an AWS PrivateLink interface endpoint for Amazon S3 in the VPC. In the route tables for the private subnets, add an entry for the interface endpoint.",
        "a": "no"
      },
      {
        "c": "Create one NAT gateway for each Availability Zone in public subnets. In each of the route tables for the private subnets, add a default route that points to the NAT gateway in the same Availability Zone.",
        "a": "no"
      }
    ],
    "nt": "Gateway endpoints for S3 provide private connectivity between VPC and S3 without using the public internet, eliminating data transfer costs and providing better security for confidential data. They are more cost-effective than NAT gateways which incur hourly charges and data processing fees."
  },
  {
    "id": 1657,
    "q": "A company wants to relocate its on-premises MySQL database to AWS. The database accepts regular imports from a client-facing application, which causes a high volume of write operations. The company is concerned that the amount of traffic might be causing performance issues within the application. How should a solutions architect design the architecture on AWS?",
    "o": [
      {
        "c": "Provision an Amazon RDS for MySQL DB instance with Provisioned IOPS SSD storage. Monitor write operation metrics by using Amazon CloudWatch. Adjust the provisioned IOPS if necessary.",
        "a": "yes"
      },
      {
        "c": "Provision an Amazon RDS for MySQL DB instance with General Purpose SSD storage. Place an Amazon ElastiCache cluster in front of the DB instance. Configure the application to query ElastiCache instead.",
        "a": "no"
      },
      {
        "c": "Provision an Amazon DocumentDB (with MongoDB compatibility) instance with a memory optimized instance type. Monitor Amazon CloudWatch for performance-related issues. Change the instance class if necessary.",
        "a": "no"
      },
      {
        "c": "Provision an Amazon Elastic File System (Amazon EFS) file system in General Purpose performance mode. Monitor Amazon CloudWatch for IOPS bottlenecks. Change to Provisioned Throughput performance mode if necessary.",
        "a": "no"
      }
    ],
    "nt": "Provisioned IOPS SSD storage is specifically designed for I/O-intensive workloads with high write volumes. It provides consistent performance and can be monitored and adjusted through CloudWatch to meet the application's requirements for high-volume write operations."
  },
  {
    "id": 1658,
    "q": "A company runs an application in the AWS Cloud that generates sensitive archival data files. The company wants to rearchitect the application's data storage. The company wants to encrypt the data files and to ensure that third parties do not have access to the data before the data is encrypted and sent to AWS. The company has already created an Amazon S3 bucket. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Configure the S3 bucket to use client-side encryption with an Amazon S3 managed encryption key. Configure the application to use the S3 bucket to store the archival files.",
        "a": "no"
      },
      {
        "c": "Configure the S3 bucket to use server-side encryption with AWS KMS keys (SSE-KMS). Configure the application to use the S3 bucket to store the archival files.",
        "a": "no"
      },
      {
        "c": "Configure the S3 bucket to use dual-layer server-side encryption with AWS KMS keys (SSE-KMS). Configure the application to use the S3 bucket to store the archival files.",
        "a": "no"
      },
      {
        "c": "Configure the application to use client-side encryption with a key stored in AWS Key Management Service (AWS KMS). Configure the application to store the archival files in the S3 bucket.",
        "a": "yes"
      }
    ],
    "nt": "Client-side encryption with AWS KMS ensures that data is encrypted before it leaves the application and is transmitted to S3. This prevents third parties from accessing unencrypted data during transmission and gives the company full control over the encryption process."
  },
  {
    "id": 1659,
    "q": "A company uses Amazon RDS with default backup settings for its database tier. The company needs to make a daily backup of the database to meet regulatory requirements. The company must retain the backups for 30 days. Which solution will meet these requirements with the LEAST operational overhead?",
    "o": [
      {
        "c": "Write an AWS Lambda function to create an RDS snapshot every day.",
        "a": "no"
      },
      {
        "c": "Modify the RDS database to have a retention period of 30 days for automated backups.",
        "a": "yes"
      },
      {
        "c": "Use AWS Systems Manager Maintenance Windows to modify the RDS backup retention period.",
        "a": "no"
      },
      {
        "c": "Create a manual snapshot every day by using the AWS CLI. Modify the RDS backup retention period.",
        "a": "no"
      }
    ],
    "nt": "RDS automated backups automatically create daily backups and retain them for the specified retention period (up to 35 days). Setting the retention period to 30 days meets the regulatory requirements with minimal operational overhead as AWS manages the backup process automatically."
  },
  {
    "id": 1660,
    "q": "A company is running a multi-tier web application on AWS. The application runs its database tier on Amazon Aurora MySQL. The application and database tiers are in the us-east-1 Region. A database administrator who regularly monitors the Aurora DB cluster finds that an intermittent increase in read traffic is creating high CPU utilization on the read replica and causing increased read latency of the application. What should a solutions architect do to improve read scalability?",
    "o": [
      {
        "c": "Reboot the Aurora DB cluster.",
        "a": "no"
      },
      {
        "c": "Create a cross-Region read replica",
        "a": "no"
      },
      {
        "c": "Increase the instance class of the read replica.",
        "a": "no"
      },
      {
        "c": "Configure Aurora Auto Scaling for the read replica.",
        "a": "yes"
      }
    ],
    "nt": "Aurora Auto Scaling automatically adjusts the number of Aurora Replicas in response to changes in read workload. This ensures that during intermittent read traffic spikes, additional replicas are provisioned to handle the load, maintaining performance without manual intervention."
  },
  {
    "id": 1661,
    "q": "A company that runs its application on AWS uses an Amazon Aurora DB cluster as its database. During peak usage hours when multiple users access and read the data, the monitoring system shows degradation of database performance for write queries. The company wants to increase the scalability of the application to meet peak usage demands. Which solution will meet these requirements MOST cost effectively?",
    "o": [
      {
        "c": "Create a second Aurora DB cluster. Configure a copy job to replicate the users' data to the new database. Update the application to use the second database to read the data.",
        "a": "no"
      },
      {
        "c": "Create an Amazon DynamoDB Accelerator (DAX) cluster in front of the existing Aurora DB cluster. Update the application to use the DAX cluster for read-only queries. Write data directly to the Aurora DB cluster.",
        "a": "no"
      },
      {
        "c": "Create an Aurora read replica in the existing Aurora DB cluster. Update the application to use the replica endpoint for read only queries and to use the cluster endpoint for write queries.",
        "a": "yes"
      },
      {
        "c": "Create an Amazon Redshift cluster. Copy the users' data to the Redshift cluster. Update the application to connect to the Redshift cluster and to perform read-only queries on the Redshift cluster.",
        "a": "no"
      }
    ],
    "nt": "Aurora read replicas provide a cost-effective way to scale read capacity by offloading read queries from the primary instance. This improves write performance on the primary instance while handling read traffic efficiently through the replica endpoint."
  },
  {
    "id": 1662,
    "q": "A company's near-real-time streaming application is running on AWS. As the data is ingested, a job runs on the data and takes 30 minutes to complete. The workload frequently experiences high latency due to large amounts of incoming data. A solutions architect needs to design a scalable and serverless solution to enhance performance. Which combination of steps should the solutions architect take? (Choose two.)",
    "o": [
      {
        "c": "Use Amazon Kinesis Data Firehose to ingest the data.",
        "a": "yes"
      },
      {
        "c": "Use AWS Lambda with AWS Step Functions to process the data.",
        "a": "no"
      },
      {
        "c": "Use AWS Database Migration Service (AWS DMS) to ingest the data.",
        "a": "no"
      },
      {
        "c": "Use Amazon EC2 instances in an Auto Scaling group to process the data.",
        "a": "no"
      },
      {
        "c": "Use AWS Fargate with Amazon Elastic Container Service (Amazon ECS) to process the data.",
        "a": "yes"
      }
    ],
    "nt": "Kinesis Data Firehose provides scalable, serverless data ingestion that can handle large volumes of streaming data. AWS Fargate with ECS offers serverless container processing that automatically scales to handle the 30-minute processing jobs without managing underlying infrastructure."
  },
  {
    "id": 1663,
    "q": "A company runs a web application on multiple Amazon EC2 instances in a VPC. The application needs to write sensitive data to an Amazon S3 bucket. The data cannot be sent over the public internet. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Create a gateway VPC endpoint for Amazon S3. Create a route in the VPC route table to the endpoint.",
        "a": "yes"
      },
      {
        "c": "Create an internal Network Load Balancer that has the S3 bucket as the target.",
        "a": "no"
      },
      {
        "c": "Deploy the S3 bucket inside the VPC. Create a route in the VPC route table to the bucket.",
        "a": "no"
      },
      {
        "c": "Create an AWS Direct Connect connection between the VPC and an S3 regional endpoint.",
        "a": "no"
      }
    ],
    "nt": "Gateway VPC endpoints provide private connectivity between VPC and S3 without traversing the public internet. They use AWS's internal network infrastructure, ensuring sensitive data remains within AWS's network and is not exposed to the public internet."
  },
  {
    "id": 1664,
    "q": "A company runs its production workload on Amazon EC2 instances with Amazon Elastic Block Store (Amazon EBS) volumes. A solutions architect needs to analyze the current EBS volume cost and to recommend optimizations. The recommendations need to include estimated monthly saving opportunities. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Use Amazon Inspector reporting to generate EBS volume recommendations for optimization.",
        "a": "no"
      },
      {
        "c": "Use AWS Systems Manager reporting to determine EBS volume recommendations for optimization.",
        "a": "no"
      },
      {
        "c": "Use Amazon CloudWatch metrics reporting to determine EBS volume recommendations for optimization.",
        "a": "no"
      },
      {
        "c": "Use AWS Compute Optimizer to generate EBS volume recommendations for optimization.",
        "a": "yes"
      }
    ],
    "nt": "AWS Compute Optimizer analyzes EBS volume configurations and usage patterns to provide specific cost optimization recommendations, including estimated monthly savings opportunities for right-sizing EBS volumes based on actual usage."
  },
  {
    "id": 1665,
    "q": "A global company runs its workloads on AWS. The company's application uses Amazon S3 buckets across AWS Regions for sensitive data storage and analysis. The company stores millions of objects in multiple S3 buckets daily. The company wants to identify all S3 buckets that are not versioning-enabled. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Use AWS Config managed rule to identify S3 bucket that is not version controlled",
        "a": "no"
      },
      {
        "c": "Use Amazon S3 Storage Lens to identify all S3 buckets that are not versioning-enabled across Regions.",
        "a": "yes"
      },
      {
        "c": "Enable IAM Access Analyzer for S3 to identify all S3 buckets that are not versioning-enabled across Regions.",
        "a": "no"
      },
      {
        "c": "Create an S3 Multi-Region Access Point to identify all S3 buckets that are not versioning-enabled across Regions.",
        "a": "no"
      }
    ],
    "nt": "Amazon S3 Storage Lens provides organization-wide visibility into S3 storage usage and configuration across all accounts and regions. It includes detailed metrics and can identify buckets that don't have versioning enabled, making it ideal for large-scale S3 management across multiple regions."
  },
  {
    "id": 1666,
    "q": "A company wants to enhance its ecommerce order-processing application that is deployed on AWS. The application must process each order exactly once without affecting the customer experience during unpredictable traffic surges. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Create an Amazon Simple Queue Service (Amazon SQS) FIFO queue. Put all the orders in the SQS queue. Configure an AWS Lambda function as the target to process the orders.",
        "a": "yes"
      },
      {
        "c": "Create an Amazon Simple Notification Service (Amazon SNS) standard topic. Publish all the orders to the SNS standard topic. Configure the application as a notification target.",
        "a": "no"
      },
      {
        "c": "Create a flow by using Amazon AppFlow. Send the orders to the flow. Configure an AWS Lambda function as the target to process the orders.",
        "a": "no"
      },
      {
        "c": "Configure AWS X-Ray in the application to track the order requests. Configure the application to process the orders by pulling the orders from Amazon CloudWatch.",
        "a": "no"
      }
    ],
    "nt": "SQS FIFO queues guarantee exactly-once processing and first-in-first-out delivery, ensuring each order is processed only once. When combined with Lambda, it provides automatic scaling during traffic surges while maintaining order integrity and customer experience."
  },
  {
    "id": 1667,
    "q": "A company has two AWS accounts: Production and Development. There are code changes ready in the Development account to push to the Production account. In the alpha phase, only two senior developers on the development team need access to the Production account. In the beta phase, more developers might need access to perform testing as well. What should a solutions architect recommend?",
    "o": [
      {
        "c": "Create two policy documents using the AWS Management Console in each account. Assign the policy to developers who need access.",
        "a": "no"
      },
      {
        "c": "Create an IAM role in the Development account. Give one IAM role access to the Production account. Allow developers to assume the role.",
        "a": "no"
      },
      {
        "c": "Create an IAM role in the Production account with the trust policy that specifies the Development account. Allow developers to assume the role.",
        "a": "yes"
      },
      {
        "c": "Create an IAM group in the Production account and add it as a principal in the trust policy that specifies the Production account. Add developers to the group.",
        "a": "no"
      }
    ],
    "nt": "Creating an IAM role in the Production account with a trust policy that allows assumption from the Development account enables secure cross-account access. Developers can assume the role when needed, and permissions can be easily managed by modifying the role's policies as requirements change from alpha to beta phases."
  },
  {
    "id": 1668,
    "q": "A company wants to restrict access to the content of its web application. The company needs to protect the content by using authorization techniques that are available on AWS. The company also wants to implement a serverless architecture for authorization and authentication that has low login latency. The solution must integrate with the web application and serve web content globally. The application currently has a small user base, but the company expects the application's user base to increase. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Configure Amazon Cognito for authentication. Implement Lambda@Edge for authorization. Configure Amazon CloudFront to serve the web application globally.",
        "a": "yes"
      },
      {
        "c": "Configure AWS Directory Service for Microsoft Active Directory for authentication. Implement AWS Lambda for authorization. Use an Application Load Balancer to serve the web application globally.",
        "a": "no"
      },
      {
        "c": "Configure Amazon Cognito for authentication. Implement AWS Lambda for authorization. Use Amazon S3 Transfer Acceleration to serve the web application globally.",
        "a": "no"
      },
      {
        "c": "Configure AWS Directory Service for Microsoft Active Directory for authentication. Implement Lambda@Edge for authorization. Use AWS Elastic Beanstalk to serve the web application globally.",
        "a": "no"
      }
    ],
    "nt": "Amazon Cognito provides scalable, serverless authentication with low latency. Lambda@Edge enables authorization at the edge locations close to users, reducing latency. CloudFront serves content globally with edge caching. This serverless architecture scales automatically as the user base grows."
  },
  {
    "id": 1669,
    "q": "A development team uses multiple AWS accounts for its development, Staging, and production environments Team members have been launching large Amazon EC2 instances that are underutilized. A solutions architect must prevent large instances from being launched in all accounts. How can the solutions architect meet this requirement with the LEAST operational overhead?",
    "o": [
      {
        "c": "Update the IAM policies to deny the launch of large EC2 instances. Apply the policies to all users",
        "a": "no"
      },
      {
        "c": "Define a resource in AWS Resource Access Manager that prevents the launch of large EC2 instances.",
        "a": "no"
      },
      {
        "c": "Create an IAM role in each account that denies the launch of large EC2 instances. Grant the developers IAM group access to the role.",
        "a": "no"
      },
      {
        "c": "Create an orgainization in AWS Organizations in the managment account with the default policy. Create a Service control prolicy that denies the launch of large EC2 instances and apply to all aws accounts.",
        "a": "yes"
      }
    ],
    "nt": "Service Control Policies (SCPs) in AWS Organizations provide centralized control over permissions across multiple accounts. By creating an SCP that denies the launch of large EC2 instances and applying it to all accounts, the solution prevents underutilized large instances with minimal operational overhead and consistent enforcement."
  },
  {
    "id": 1670,
    "q": "A company has migrated a fleet of hundreds of on-premises virtual machines (VMs) to Amazon EC2 instances. The instances run a diverse fleet of Windows Server versions along with several Linux distributions. The company wants a solution that will automate inventory and updates of the operating systems. The company also needs a summary of common vulnerabilities of each instance for regular monthly reviews. What should a solutions architect recommend to meet these requirements?",
    "o": [
      {
        "c": "Set up AWS Systems Manager Patch Manager to manage all the EC2 instances. Configure AWS Security Hub to produce monthly reports.",
        "a": "no"
      },
      {
        "c": "Set up AWS Systems Manager Patch Manager to manage all the EC2 instances. Deploy Amazon Inspector, and configure monthly reports.",
        "a": "yes"
      },
      {
        "c": "Set up AWS Shield Advanced, and configure monthly reports. Deploy AWS Config to automate patch installations on the EC2 instances.",
        "a": "no"
      },
      {
        "c": "Set up Amazon GuardDuty in the account to monitor all EC2 instances. Deploy AWS Config to automate patch installations on the EC2 instances.",
        "a": "no"
      }
    ],
    "nt": "AWS Systems Manager Patch Manager automates operating system patching across diverse Windows and Linux instances. Amazon Inspector assesses instances for vulnerabilities and generates detailed reports, providing the required monthly vulnerability summaries for review."
  },
  {
    "id": 1671,
    "q": "A company hosts its application in the AWS Cloud. The application runs on Amazon EC2 instances behind an Elastic Load Balancer in an Auto Scaling group and with an Amazon DynamoDB table. The company wants to ensure the application can be made available in another AWS Region with minimal downtime. What should a solutions architect do to meet these requirements with the LEAST amount of downtime?",
    "o": [
      {
        "c": "Create an Auto Scaling group and a load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.",
        "a": "yes"
      },
      {
        "c": "Create an AWS CloudFormation template to create EC2 instances, load balancers, and DynamoDB tables to be executed when needed. Configure DNS failover to point to the new disaster recovery Region's load balancer.",
        "a": "no"
      },
      {
        "c": "Create an AWS CloudFormation template to create EC2 instances and a load balancer to be executed when needed. Configure the DynamoDB table as a global table. Configure DNS failover to point to the new disaster recovery Region's load balancer.",
        "a": "no"
      },
      {
        "c": "Create an Auto Scaling group and load balancer in the disaster recovery Region. Configure the DynamoDB table as a global table. Create an Amazon CloudWatch alarm to trigger and AWS Lambda function that updates Amazon Route 53 pointing to the disaster recovery load balancer.",
        "a": "no"
      }
    ],
    "nt": "DynamoDB global tables provide multi-region replication with automatic conflict resolution, ensuring data is available in the DR region. Pre-provisioning Auto Scaling groups and load balancers in the DR region, combined with DNS failover, enables rapid failover with minimal downtime during a regional outage."
  },
  {
    "id": 1672,
    "q": "A company runs an application on Amazon EC2 instances in a private subnet. The application needs to store and retrieve data in Amazon S3 buckets. According to regulatory requirements, the data must not travel across the public internet. What should a solutions architect do to meet these requirements MOST cost-effectively?",
    "o": [
      {
        "c": "Deploy a NAT gateway to access the S3 buckets.",
        "a": "no"
      },
      {
        "c": "Deploy AWS Storage Gateway to access the S3 buckets.",
        "a": "no"
      },
      {
        "c": "Deploy an S3 interface endpoint to access the S3 buckets.",
        "a": "no"
      },
      {
        "c": "Deploy an S3 gateway endpoint to access the S3 buckets.",
        "a": "yes"
      }
    ],
    "nt": "S3 gateway endpoints provide private connectivity between VPC and S3 without using the public internet, meeting regulatory requirements. They are more cost-effective than NAT gateways (which incur hourly charges) and interface endpoints (which have hourly and data processing charges), as gateway endpoints have no additional costs beyond standard S3 operations."
  },
  {
    "id": 1673,
    "q": "A company hosts an application on Amazon EC2 instances that run in a single Availability Zone. The application is accessible by using the transport layer of the Open Systems Interconnection (OSI) model. The company needs the application architecture to have high availability. Which combination of steps will meet these requirements MOST cost-effectively? (Choose two.)",
    "o": [
      {
        "c": "Configure new EC2 instances in a different Availability Zone. Use Amazon Route 53 to route traffic to all instances.",
        "a": "no"
      },
      {
        "c": "Configure a Network Load Balancer in front of the EC2 instances.",
        "a": "yes"
      },
      {
        "c": "Configure a Network Load Balancer for TCP traffic to the instances. Configure an Application Load Balancer for HTTP and HTTPS traffic to the instances.",
        "a": "no"
      },
      {
        "c": "Create an Auto Scaling group for the EC2 instances. Configure the Auto Scaling group to use multiple Availability Zones. Configure the Auto Scaling group to run application health checks on the instances.",
        "a": "yes"
      },
      {
        "c": "Create an Amazon CloudWatch alarm. Configure the alarm to restart EC2 instances that transition to a stopped state.",
        "a": "no"
      }
    ],
    "nt": "A Network Load Balancer operates at the transport layer (Layer 4) and distributes TCP traffic across instances in multiple Availability Zones. An Auto Scaling group across multiple AZs ensures instances are distributed for high availability and can automatically replace failed instances, providing cost-effective high availability for transport layer applications."
  },
  {
    "id": 1674,
    "q": "A company hosts its static website by using Amazon S3. The company wants to add a contact form to its webpage. The contact form will have dynamic server-side components for users to input their name, email address, phone number, and user message. The company anticipates that there will be fewer than 100 site visits each month. Which solution will meet these requirements MOST cost-effectively?",
    "o": [
      {
        "c": "Host the dynamic contact form in Amazon Elastic Container Service (Amazon ECS). Set up Amazon Simple Email Service (Amazon SES) to connect to a third-party email provider.",
        "a": "no"
      },
      {
        "c": "Create an Amazon API Gateway endpoint that returns the contact form from an AWS Lambda function. Configure another Lambda function on the API Gateway to publish a message to an Amazon Simple Notification Service (Amazon SNS) topic.",
        "a": "yes"
      },
      {
        "c": "Host the website by using AWS Amplify Hosting for static content and dynamic content. Use server-side scripting to build the contact form. Configure Amazon Simple Queue Service (Amazon SQS) to deliver the message to the company.",
        "a": "no"
      },
      {
        "c": "Migrate the website from Amazon S3 to Amazon EC2 instances that run Windows Server. Use Internet Information Services (IIS) for Windows Server to host the webpage. Use client-side scripting to build the contact form. Integrate the form with Amazon WorkMail.",
        "a": "no"
      }
    ],
    "nt": "API Gateway with Lambda provides a serverless solution for dynamic form processing that scales to zero when not in use, making it extremely cost-effective for low-traffic websites. SNS efficiently delivers form submissions without maintaining persistent infrastructure, minimizing costs for infrequent usage."
  },
  {
    "id": 1675,
    "q": "A company uses AWS Organizations to create dedicated AWS accounts for each business unit to manage each business unit's account independently upon request. The root email recipient missed a notification that was sent to the root user email address of one account. The company wants to ensure that all future notifications are not missed. Future notifications must be limited to account administrators. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Configure the company's email server to forward notification email messages that are sent to the AWS account root user email address to all users in the organization.",
        "a": "no"
      },
      {
        "c": "Configure all AWS account root user email addresses as distribution lists that go to a few administrators who can respond to alerts. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.",
        "a": "yes"
      },
      {
        "c": "Configure all AWS account root user email messages to be sent to one administrator who is responsible for monitoring alerts and forwarding those alerts to the appropriate groups.",
        "a": "no"
      },
      {
        "c": "Configure all existing AWS accounts and all newly created accounts to use the same root user email address. Configure AWS account alternate contacts in the AWS Organizations console or programmatically.",
        "a": "no"
      }
    ],
    "nt": "Configuring root email addresses as distribution lists ensures multiple administrators receive notifications. Setting alternate contacts in AWS Organizations ensures specific types of notifications (billing, operations, security) go to the appropriate teams, preventing missed alerts while maintaining proper access control."
  },
  {
    "id": 1676,
    "q": "A company runs an ecommerce application on AWS. Amazon EC2 instances process purchases and store the purchase details in an Amazon Aurora PostgreSQL DB cluster. Customers are experiencing application timeouts during times of peak usage. A solutions architect needs to rearchitect the application so that the application can scale to meet peak usage demands. Which combination of actions will meet these requirements MOST cost-effectively? (Choose two.)",
    "o": [
      {
        "c": "Configure an Auto Scaling group of new EC2 instances to retry the purchases until the processing is complete. Update the applications to connect to the DB cluster by using Amazon RDS Proxy.",
        "a": "no"
      },
      {
        "c": "Configure the application to use an Amazon ElastiCache cluster in front of the Aurora PostgreSQL DB cluster.",
        "a": "no"
      },
      {
        "c": "Update the application to send the purchase requests to an Amazon Simple Queue Service (Amazon SQS) queue. Configure an Auto Scaling group of new EC2 instances that read from the SQS queue.",
        "a": "yes"
      },
      {
        "c": "Configure an AWS Lambda function to retry the ticket purchases until the processing is complete.",
        "a": "no"
      },
      {
        "c": "Configure an Amazon AP! Gateway REST API with a usage plan.",
        "a": "no"
      }
    ],
    "nt": "SQS queues decouple purchase processing from the web tier, allowing requests to be buffered during peak traffic and processed as capacity becomes available. Auto Scaling groups automatically adjust EC2 instance count based on queue depth, providing cost-effective scaling to handle peak loads without over-provisioning."
  },
  {
    "id": 1677,
    "q": "A company that uses AWS Organizations runs 150 applications across 30 different AWS accounts. The company used AWS Cost and Usage Report to create a new report in the management account. The report is delivered to an Amazon S3 bucket that is replicated to a bucket in the data collection account. The company's senior leadership wants to view a custom dashboard that provides NAT gateway costs each day starting at the beginning of the current month. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Share an Amazon QuickSight dashboard that includes the requested table visual. Configure QuickSight to use AWS DataSync to query the new report.",
        "a": "no"
      },
      {
        "c": "Share an Amazon QuickSight dashboard that includes the requested table visual. Configure QuickSight to use Amazon Athena to query the new report.",
        "a": "yes"
      },
      {
        "c": "Share an Amazon CloudWatch dashboard that includes the requested table visual. Configure CloudWatch to use AWS DataSync to query the new report.",
        "a": "no"
      },
      {
        "c": "Share an Amazon CloudWatch dashboard that includes the requested table visual. Configure CloudWatch to use Amazon Athena to query the new report.",
        "a": "no"
      }
    ],
    "nt": "Amazon Athena can directly query Cost and Usage Reports stored in S3 using standard SQL. QuickSight integrates seamlessly with Athena to create interactive dashboards and visualizations, making it ideal for analyzing NAT gateway costs and other cost data across multiple accounts."
  },
  {
    "id": 1678,
    "q": "A company is hosting a high-traffic static website on Amazon S3 with an Amazon CloudFront distribution that has a default TTL of 0 seconds. The company wants to implement caching to improve performance for the website. However, the company also wants to ensure that stale content is not served for more than a few minutes after a deployment. Which combination of caching methods should a solutions architect implement to meet these requirements? (Choose two.)",
    "o": [
      {
        "c": "Set the CloudFront default TTL to 2 minutes.",
        "a": "yes"
      },
      {
        "c": "Set a default TTL of 2 minutes on the S3 bucket.",
        "a": "no"
      },
      {
        "c": "Add a Cache-Control private directive to the objects in Amazon S3.",
        "a": "no"
      },
      {
        "c": "Create an AWS Lambda@Edge function to add an Expires header to HTTP responses. Configure the function to run on viewer response.",
        "a": "no"
      },
      {
        "c": "Add a Cache-Control max-age directive of 24 hours to the objects in Amazon S3. On deployment, create a CloudFront invalidation to clear any changed files from edge caches.",
        "a": "yes"
      }
    ],
    "nt": "Setting CloudFront TTL to 2 minutes ensures content is cached briefly at edge locations, improving performance while limiting staleness. Using Cache-Control max-age with CloudFront invalidations provides long-term caching benefits while allowing immediate content updates during deployments by clearing specific objects from cache."
  },
  {
    "id": 1679,
    "q": "A company uses Amazon EC2 instances and AWS Lambda functions to run its application. The company has VPCs with public subnets and private subnets in its AWS account. The EC2 instances run in a private subnet in one of the VPCs. The Lambda functions need direct network access to the EC2 instances for the application to work. The application will run for at least 1 year. The company expects the number of Lambda functions that the application uses to increase during that time. The company wants to maximize its savings on all application resources and to keep network latency between the services low. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Purchase an EC2 Instance Savings Plan Optimize the Lambda functions' duration and memory usage and the number of invocations. Connect the Lambda functions to the private subnet that contains the EC2 instances.",
        "a": "no"
      },
      {
        "c": "Purchase an EC2 Instance Savings Plan Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to a public subnet in the same VPC where the EC2 instances run.",
        "a": "no"
      },
      {
        "c": "Purchase a Compute Savings Plan. Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Connect the Lambda functions to the private subnet that contains the EC2 instances.",
        "a": "yes"
      },
      {
        "c": "Purchase a Compute Savings Plan. Optimize the Lambda functions' duration and memory usage, the number of invocations, and the amount of data that is transferred. Keep the Lambda functions in the Lambda service VPC.",
        "a": "no"
      }
    ],
    "nt": "Compute Savings Plans provide the broadest savings coverage across EC2, Lambda, and Fargate usage, maximizing savings as Lambda usage increases. Connecting Lambda functions to the same private subnet as EC2 instances minimizes network latency by keeping communication within the same VPC and availability zone."
  },
  {
    "id": 1680,
    "q": "A company hosts a data lake on AWS. The data lake consists of data in Amazon S3 and Amazon RDS for PostgreSQL. The company needs a reporting solution that provides data visualization and includes all the data sources within the data lake. Only the company's management team should have full access to all the visualizations. The rest of the company should have only limited access. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate IAM roles.",
        "a": "yes"
      },
      {
        "c": "Create an analysis in Amazon QuickSight. Connect all the data sources and create new datasets. Publish dashboards to visualize the data. Share the dashboards with the appropriate users and groups.",
        "a": "no"
      },
      {
        "c": "Create an AWS Glue table and crawler for the data in Amazon S3. Create an AWS Glue extract, transform, and load (ETL) job to produce reports. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.",
        "a": "no"
      },
      {
        "c": "Create an AWS Glue table and crawler for the data in Amazon S3. Use Amazon Athena Federated Query to access data within Amazon RDS for PostgreSQL. Generate reports by using Amazon Athena. Publish the reports to Amazon S3. Use S3 bucket policies to limit access to the reports.",
        "a": "no"
      }
    ],
    "nt": "Amazon QuickSight can connect to multiple data sources including S3 and RDS, creating unified visualizations. IAM role-based sharing provides granular access control, allowing full access for management while restricting access for other users through role-based permissions."
  },
  {
    "id": 1681,
    "q": "A company runs an ecommerce application on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. The Auto Scaling group scales based on CPU utilization metrics. The ecommerce application stores the transaction data in a MySQL 8.0 database that is hosted on a large EC2 instance. The database's performance degrades quickly as application load increases. The application handles more read requests than write transactions. The company wants a solution that will automatically scale the database to meet the demand of unpredictable read workloads while maintaining high availability. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Use Amazon Redshift with a single node for leader and compute functionality.",
        "a": "no"
      },
      {
        "c": "Use Amazon RDS with a Single-AZ deployment Configure Amazon RDS to add reader instances in a different Availability Zone.",
        "a": "no"
      },
      {
        "c": "Use Amazon Aurora with a Multi-AZ deployment. Configure Aurora Auto Scaling with Aurora Replicas.",
        "a": "yes"
      },
      {
        "c": "Use Amazon ElastiCache for Memcached with EC2 Spot Instances.",
        "a": "no"
      }
    ],
    "nt": "Amazon Aurora with Multi-AZ deployment provides high availability through automatic failover. Aurora Auto Scaling automatically adds and removes read replicas based on workload, efficiently handling unpredictable read traffic while maintaining performance and availability for read-heavy applications."
  },
  {
    "id": 1682,
    "q": "A company has an application that ingests incoming messages. Dozens of other applications and microservices then quickly consume these messages. The number of messages varies drastically and sometimes increases suddenly to 100,000 each second. The company wants to decouple the solution and increase scalability. Which solution meets these requirements?",
    "o": [
      {
        "c": "Persist the messages to Amazon Kinesis Data Analytics. Configure the consumer applications to read and process the messages.",
        "a": "no"
      },
      {
        "c": "Deploy the ingestion application on Amazon EC2 instances in an Auto Scaling group to scale the number of EC2 instances based on CPU metrics.",
        "a": "no"
      },
      {
        "c": "Write the messages to Amazon Kinesis Data Streams with a single shard. Use an AWS Lambda function to preprocess messages and store them in Amazon DynamoDB. Configure the consumer applications to read from DynamoDB to process the messages.",
        "a": "no"
      },
      {
        "c": "Publish the messages to an Amazon Simple Notification Service (Amazon SNS) topic with multiple Amazon Simple Queue Service (Amazon SOS) subscriptions. Configure the consumer applications to process the messages from the queues.",
        "a": "yes"
      }
    ],
    "nt": "SNS topics with multiple SQS subscriptions provide a highly scalable pub/sub messaging pattern that can handle sudden traffic spikes to 100,000 messages per second. This decouples message producers from consumers and allows multiple applications to process messages independently at their own pace."
  },
  {
    "id": 1683,
    "q": "An application development team is designing a microservice that will convert large images to smaller, compressed images. When a user uploads an image through the web interface, the microservice should store the image in an Amazon S3 bucket, process and compress the image with an AWS Lambda function, and store the image in its compressed form in a different S3 bucket. A solutions architect needs to design a solution that uses durable, stateless components to process the images automatically. Which combination of actions will meet these requirements? (Choose two.)",
    "o": [
      {
        "c": "Create an Amazon Simple Queue Service (Amazon SQS) queue. Configure the S3 bucket to send a notification to the SQS queue when an image is uploaded to the S3 bucket.",
        "a": "yes"
      },
      {
        "c": "Configure the Lambda function to use the Amazon Simple Queue Service (Amazon SQS) queue as the invocation source. When the SQS message is successfully processed, delete the message in the queue.",
        "a": "yes"
      },
      {
        "c": "Configure the Lambda function to monitor the S3 bucket for new uploads. When an uploaded image is detected, write the file name to a text file in memory and use the text file to keep track of the images that were processed.",
        "a": "no"
      },
      {
        "c": "Launch an Amazon EC2 instance to monitor an Amazon Simple Queue Service (Amazon SQS) queue. When items are added to the queue, log the file name in a text file on the EC2 instance and invoke the Lambda function.",
        "a": "no"
      },
      {
        "c": "Configure an Amazon EventBridge (Amazon CloudWatch Events) event to monitor the S3 bucket. When an image is uploaded, send an alert to an Amazon ample Notification Service (Amazon SNS) topic with the application owner's email address for further processing.",
        "a": "no"
      }
    ],
    "nt": "SQS queues provide durable message storage that ensures no image processing requests are lost. Lambda with SQS as an event source automatically processes messages and handles retries for failed processing. This creates a stateless, serverless architecture that automatically scales with the workload and ensures reliable image processing."
  },
  {
    "id": 1684,
    "q": "A company has a three-tier web application that is deployed on AWS. The web servers are deployed in a public subnet in a VPC. The application servers and database servers are deployed in private subnets in the same VPC. The company has deployed a third-party virtual firewall appliance from AWS Marketplace in an inspection VPC. The appliance is configured with an IP interface that can accept IP packets. A solutions architect needs to integrate the web application with the appliance to inspect all traffic to the application before the traffic reaches the web server. Which solution will meet these requirements with the LEAST operational overhead?",
    "o": [
      {
        "c": "Create a Network Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.",
        "a": "no"
      },
      {
        "c": "Create an Application Load Balancer in the public subnet of the application's VPC to route the traffic to the appliance for packet inspection.",
        "a": "no"
      },
      {
        "c": "Deploy a transit gateway in the inspection VPConfigure route tables to route the incoming packets through the transit gateway.",
        "a": "no"
      },
      {
        "c": "Deploy a Gateway Load Balancer in the inspection VPC. Create a Gateway Load Balancer endpoint to receive the incoming packets and forward the packets to the appliance.",
        "a": "yes"
      }
    ],
    "nt": "Gateway Load Balancer is specifically designed for deploying, scaling, and managing third-party virtual appliances like firewalls. It uses the GENEVE protocol on port 6081 to distribute traffic across multiple firewall instances, providing automatic scaling and simplified management of security appliances with minimal operational overhead."
  },
  {
    "id": 1685,
    "q": "A company wants to improve its ability to clone large amounts of production data into a test environment in the same AWS Region. The data is stored in Amazon EC2 instances on Amazon Elastic Block Store (Amazon EBS) volumes. Modifications to the cloned data must not affect the production environment. The software that accesses this data requires consistently high I/O performance. A solutions architect needs to minimize the time that is required to clone the production data into the test environment. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Take EBS snapshots of the production EBS volumes. Restore the snapshots onto EC2 instance store volumes in the test environment.",
        "a": "no"
      },
      {
        "c": "Configure the production EBS volumes to use the EBS Multi-Attach feature. Take EBS snapshots of the production EBS volumes. Attach the production EBS volumes to the EC2 instances in the test environment.",
        "a": "no"
      },
      {
        "c": "Take EBS snapshots of the production EBS volumes. Create and initialize new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment before restoring the volumes from the production EBS snapshots.",
        "a": "no"
      },
      {
        "c": "Take EBS snapshots of the production EBS volumes. Turn on the EBS fast snapshot restore feature on the EBS snapshots. Restore the snapshots into new EBS volumes. Attach the new EBS volumes to EC2 instances in the test environment.",
        "a": "yes"
      }
    ],
    "nt": "EBS fast snapshot restore eliminates the performance penalty typically associated with initial access to restored EBS volumes. By pre-warming the blocks and making them immediately available at full performance, it significantly reduces the time required to clone production data while ensuring high I/O performance for the test environment."
  },
  {
    "id": 1686,
    "q": "An ecommerce company wants to launch a one-deal-a-day website on AWS. Each day will feature exactly one product on sale for a period of 24 hours. The company wants to be able to handle millions of requests each hour with millisecond latency during peak hours. Which solution will meet these requirements with the LEAST operational overhead?",
    "o": [
      {
        "c": "Use Amazon S3 to host the full website in different S3 buckets. Add Amazon CloudFront distributions. Set the S3 buckets as origins for the distributions. Store the order data in Amazon S3.",
        "a": "no"
      },
      {
        "c": "Deploy the full website on Amazon EC2 instances that run in Auto Scaling groups across multiple Availability Zones. Add an Application Load Balancer (ALB) to distribute the website traffic. Add another ALB for the backend APIs. Store the data in Amazon RDS for MySQL.",
        "a": "no"
      },
      {
        "c": "Migrate the full application to run in containers. Host the containers on Amazon Elastic Kubernetes Service (Amazon EKS). Use the Kubernetes Cluster Autoscaler to increase and decrease the number of pods to process bursts in traffic. Store the data in Amazon RDS for MySQL.",
        "a": "no"
      },
      {
        "c": "Use an Amazon S3 bucket to host the website's static content. Deploy an Amazon CloudFront distribution. Set the S3 bucket as the origin. Use Amazon API Gateway and AWS Lambda functions for the backend APIs. Store the data in Amazon DynamoDB.",
        "a": "yes"
      }
    ],
    "nt": "This serverless architecture provides massive scalability with minimal operational overhead. CloudFront and S3 handle static content with global edge caching for millisecond latency. API Gateway and Lambda automatically scale to handle millions of requests. DynamoDB provides single-digit millisecond latency for dynamic data with automatic scaling."
  },
  {
    "id": 1687,
    "q": "A solutions architect is using Amazon S3 to design the storage architecture of a new digital media application. The media files must be resilient to the loss of an Availability Zone. Some files are accessed frequently while other files are rarely accessed in an unpredictable pattern. The solutions architect must minimize the costs of storing and retrieving the media files. Which storage option meets these requirements?",
    "o": [
      {
        "c": "S3 Standard.",
        "a": "no"
      },
      {
        "c": "S3 Intelligent-Tiering.",
        "a": "yes"
      },
      {
        "c": "S3 Standard-Infrequent Access (S3 Standard-IA).",
        "a": "no"
      },
      {
        "c": "S3 One Zone-Infrequent Access (S3 One Zone-IA).",
        "a": "no"
      }
    ],
    "nt": "S3 Intelligent-Tiering automatically moves objects between frequent and infrequent access tiers based on access patterns, optimizing costs for unpredictable access patterns. It maintains resilience across multiple Availability Zones unlike S3 One Zone-IA, and avoids retrieval fees of S3 Standard-IA for frequently accessed objects."
  },
  {
    "id": 1688,
    "q": "A company is storing backup files by using Amazon S3 Standard storage. The files are accessed frequently for 1 month. However, the files are not accessed after 1 month. The company must keep the files indefinitely. Which storage solution will meet these requirements MOST cost-effectively?",
    "o": [
      {
        "c": "Configure S3 Intelligent-Tiering to automatically migrate objects.",
        "a": "no"
      },
      {
        "c": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Glacier Deep Archive after 1 month.",
        "a": "yes"
      },
      {
        "c": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 Standard-Infrequent Access (S3 Standard-IA) after 1 month.",
        "a": "no"
      },
      {
        "c": "Create an S3 Lifecycle configuration to transition objects from S3 Standard to S3 One Zone-Infrequent Access (S3 One Zone-IA) after 1 month.",
        "a": "no"
      }
    ],
    "nt": "S3 Glacier Deep Archive provides the lowest-cost storage option for long-term data retention where immediate access is not required. For backup files that are rarely accessed after 1 month but must be kept indefinitely, this provides the most cost-effective solution while maintaining durability and availability."
  },
  {
    "id": 1689,
    "q": "A company observes an increase in Amazon EC2 costs in its most recent bill. The billing team notices unwanted vertical scaling of instance types for a couple of EC2 instances. A solutions architect needs to create a graph comparing the last 2 months of EC2 costs and perform an in-depth analysis to identify the root cause of the vertical scaling. How should the solutions architect generate the information with the LEAST operational overhead?",
    "o": [
      {
        "c": "Use AWS Budgets to create a budget report and compare EC2 costs based on instance types.",
        "a": "no"
      },
      {
        "c": "Use Cost Explorer's granular filtering feature to perform an in-depth analysis of EC2 costs based on instance types.",
        "a": "yes"
      },
      {
        "c": "Use graphs from the AWS Billing and Cost Management dashboard to compare EC2 costs based on instance types for the last 2 months.",
        "a": "no"
      },
      {
        "c": "Use AWS Cost and Usage Reports to create a report and send it to an Amazon S3 bucket. Use Amazon QuickSight with Amazon S3 as a source to generate an interactive graph based on instance types.",
        "a": "no"
      }
    ],
    "nt": "AWS Cost Explorer provides built-in granular filtering capabilities that allow detailed analysis of EC2 costs by instance type, time period, and other dimensions. It requires no setup or additional services, providing immediate visibility into cost changes with minimal operational overhead."
  },
  {
    "id": 1690,
    "q": "A company is designing an application. The application uses an AWS Lambda function to receive information through Amazon API Gateway and to store the information in an Amazon Aurora PostgreSQL database. During the proof-of-concept stage, the company has to increase the Lambda quotas significantly to handle the high volumes of data that the company needs to load into the database. A solutions architect must recommend a new design to improve scalability and minimize the configuration effort. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Refactor the Lambda function code to Apache Tomcat code that runs on Amazon EC2 instances. Connect the database by using native Java Database Connectivity (JDBC) drivers.",
        "a": "no"
      },
      {
        "c": "Change the platform from Aurora to Amazon DynamoDProvision a DynamoDB Accelerator (DAX) cluster. Use the DAX client SDK to point the existing DynamoDB API calls at the DAX cluster.",
        "a": "no"
      },
      {
        "c": "Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using Amazon Simple Notification",
        "a": "no"
      },
      {
        "c": "Set up two Lambda functions. Configure one function to receive the information. Configure the other function to load the information into the database. Integrate the Lambda functions by using an Amazon Simple Queue Service (Amazon SQS) queue.",
        "a": "yes"
      }
    ],
    "nt": "Using SQS queues to decouple the receiving and processing Lambda functions provides automatic buffering and scaling. The receiving function can handle high volumes of incoming requests and place them in the queue, while the processing function can scale independently to consume messages from the queue, eliminating the need for quota increases and simplifying scalability."
  },
  {
    "id": 1691,
    "q": "A company needs to review its AWS Cloud deployment to ensure that its Amazon S3 buckets do not have unauthorized configuration changes. What should a solutions architect do to accomplish this goal?",
    "o": [
      {
        "c": "Turn on AWS Config with the appropriate rules.",
        "a": "yes"
      },
      {
        "c": "Turn on AWS Trusted Advisor with the appropriate checks.",
        "a": "no"
      },
      {
        "c": "Turn on Amazon Inspector with the appropriate assessment template.",
        "a": "no"
      },
      {
        "c": "Turn on Amazon S3 server access logging. Configure Amazon EventBridge (Amazon Cloud Watch Events).",
        "a": "no"
      }
    ],
    "nt": "AWS Config continuously monitors and records AWS resource configurations, including S3 bucket settings. It can detect unauthorized configuration changes and evaluate resources against desired configurations using rules, providing comprehensive compliance monitoring for S3 bucket configurations."
  },
  {
    "id": 1692,
    "q": "A company is launching a new application and will display application metrics on an Amazon CloudWatch dashboard. The company's product manager needs to access this dashboard periodically. The product manager does not have an AWS account. A solutions architect must provide access to the product manager by following the principle of least privilege. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Share the dashboard from the CloudWatch console. Enter the product manager's email address, and complete the sharing steps. Provide a shareable link for the dashboard to the product manager.",
        "a": "yes"
      },
      {
        "c": "Create an IAM user specifically for the product manager. Attach the CloudWatchReadOnlyAccess AWS managed policy to the user. Share the new login credentials with the product manager. Share the browser URL of the correct dashboard with the product manager.",
        "a": "no"
      },
      {
        "c": "Create an IAM user for the company's employees. Attach the ViewOnlyAccess AWS managed policy to the IAM user. Share the new login credentials with the product manager. Ask the product manager to navigate to the CloudWatch console and locate the dashboard by name in the Dashboards section.",
        "a": "no"
      },
      {
        "c": "Deploy a bastion server in a public subnet. When the product manager requires access to the dashboard, start the server and share the RDP credentials. On the bastion server, ensure that the browser is configured to open the dashboard URL with cached AWS credentials that have appropriate permissions to view the dashboard.",
        "a": "no"
      }
    ],
    "nt": "CloudWatch dashboard sharing allows external users to view dashboards without AWS accounts or credentials. This follows the principle of least privilege by providing read-only access to specific dashboards only, without granting any other AWS permissions or requiring account management."
  },
  {
    "id": 1693,
    "q": "A company is migrating applications to AWS. The applications are deployed in different accounts. The company manages the accounts centrally by using AWS Organizations. The company's security team needs a single sign-on (SSO) solution across all the company's accounts. The company must continue managing the users and groups in its on-premises self-managed Microsoft Active Directory. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a one-way forest trust or a one-way domain trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.",
        "a": "no"
      },
      {
        "c": "Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console. Create a two-way forest trust to connect the company's self-managed Microsoft Active Directory with AWS SSO by using AWS Directory Service for Microsoft Active Directory.",
        "a": "yes"
      },
      {
        "c": "Use AWS Directory Service. Create a two-way trust relationship with the company's self-managed Microsoft Active Directory.",
        "a": "no"
      },
      {
        "c": "Deploy an identity provider (IdP) on premises. Enable AWS Single Sign-On (AWS SSO) from the AWS SSO console.",
        "a": "no"
      }
    ],
    "nt": "AWS SSO with a two-way forest trust to on-premises Active Directory enables centralized identity management while maintaining existing AD infrastructure. This provides seamless single sign-on across all AWS accounts in the organization while allowing continued use of the company's self-managed Active Directory for user and group management."
  },
  {
    "id": 1694,
    "q": "A company provides a Voice over Internet Protocol (VoIP) service that uses UDP connections. The service consists of Amazon EC2 instances that run in an Auto Scaling group. The company has deployments across multiple AWS Regions. The company needs to route users to the Region with the lowest latency. The company also needs automated failover between Regions. Which solution will meet these requirements?",
    "o": [
      {
        "c": "Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Use the NLB as an AWS Global Accelerator endpoint in each Region.",
        "a": "yes"
      },
      {
        "c": "Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Use the ALB as an AWS Global Accelerator endpoint in each Region.",
        "a": "no"
      },
      {
        "c": "Deploy a Network Load Balancer (NLB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 latency record that points to aliases for each NLB. Create an Amazon CloudFront distribution that uses the latency record as an origin.",
        "a": "no"
      },
      {
        "c": "Deploy an Application Load Balancer (ALB) and an associated target group. Associate the target group with the Auto Scaling group. Create an Amazon Route 53 weighted record that points to aliases for each ALB. Deploy an Amazon CloudFront distribution that uses the weighted record as an origin.",
        "a": "no"
      }
    ],
    "nt": "AWS Global Accelerator uses the AWS global network to route traffic to optimal endpoints based on real-time network conditions, providing low latency for UDP-based VoIP traffic. It automatically fails over between regions and works with Network Load Balancers which support UDP traffic and preserve client IP addresses."
  },
  {
    "id": 1695,
    "q": "A development team runs monthly resource-intensive tests on its general purpose Amazon RDS for MySQL DB instance with Performance Insights enabled. The testing lasts for 48 hours once a month and is the only process that uses the database. The team wants to reduce the cost of running the tests without reducing the compute and memory attributes of the DB instance. Which solution meets these requirements MOST cost-effectively?",
    "o": [
      {
        "c": "Stop the DB instance when tests are completed. Restart the DB instance when required.",
        "a": "no"
      },
      {
        "c": "Use an Auto Scaling policy with the DB instance to automatically scale when tests are completed.",
        "a": "no"
      },
      {
        "c": "Create a snapshot when tests are completed. Terminate the DB instance and restore the snapshot when required.",
        "a": "yes"
      },
      {
        "c": "Modify the DB instance to a low-capacity instance when tests are completed. Modify the DB instance again when required.",
        "a": "no"
      }
    ],
    "nt": "Creating snapshots and terminating the DB instance when not in use eliminates ongoing compute costs while preserving the database state. Snapshots are stored cost-effectively in S3, and the database can be quickly restored from snapshot when needed for monthly testing, providing significant cost savings for infrequently used databases."
  },
  {
    "id": 1696,
    "q": "A company that hosts its web application on AWS wants to ensure all Amazon EC2 instances, Amazon RDS DB instances, and Amazon Redshift clusters are configured with tags. The company wants to minimize the effort of configuring and operating this check. What should a solutions architect do to accomplish this?",
    "o": [
      {
        "c": "Use AWS Config rules to define and detect resources that are not properly tagged.",
        "a": "yes"
      },
      {
        "c": "Use Cost Explorer to display resources that are not properly tagged. Tag those resources manually.",
        "a": "no"
      },
      {
        "c": "Write API calls to check all resources for proper tag allocation. Periodically run the code on an EC2 instance.",
        "a": "no"
      },
      {
        "c": "Write API calls to check all resources for proper tag allocation. Schedule an AWS Lambda function through Amazon CloudWatch to periodically run the code.",
        "a": "no"
      }
    ],
    "nt": "AWS Config provides managed rules for tag compliance that automatically check if resources are properly tagged according to defined standards. This requires minimal configuration and operational effort compared to custom solutions, and provides continuous compliance monitoring across multiple resource types."
  },
  {
    "id": 1697,
    "q": "A company runs an online marketplace web application on AWS. The application serves hundreds of thousands of users during peak hours. The company needs a scalable, near-real-time solution to share the details of millions of financial transactions with several other internal applications. Transactions also need to be processed to remove sensitive data before being stored in a document database for low-latency retrieval. What should a solutions architect recommend to meet these requirements?",
    "o": [
      {
        "c": "Store the transactions data into Amazon DynamoDB. Set up a rule in DynamoDB to remove sensitive data from every transaction upon write. Use DynamoDB Streams to share the transactions data with other applications.",
        "a": "no"
      },
      {
        "c": "Stream the transactions data into Amazon Kinesis Data Firehose to store data in Amazon DynamoDB and Amazon S3. Use AWS Lambda integration with Kinesis Data Firehose to remove sensitive data. Other applications can consume the data stored in Amazon S3.",
        "a": "no"
      },
      {
        "c": "Stream the transactions data into Amazon Kinesis Data Streams. Use AWS Lambda integration to remove sensitive data from every transaction and then store the transactions data in AmazonDynamoDB. Other applications can consume the transactions data off the Kinesis data stream.",
        "a": "yes"
      },
      {
        "c": "Store the batched transactions data in Amazon S3 as files. Use AWS Lambda to process every file and remove sensitive data before updating the files in Amazon S3. The Lambda function then stores the data in Amazon DynamoDB. Other applications can consume transaction files stored in Amazon S3.",
        "a": "no"
      }
    ],
    "nt": "Kinesis Data Streams can handle millions of transactions in near-real-time with high throughput. Lambda integration enables real-time processing to remove sensitive data before storage. Multiple applications can independently consume from the same Kinesis stream, and DynamoDB provides low-latency retrieval for the processed data."
  },
  {
    "id": 1698,
    "q": "A company is preparing to launch a public-facing web application in the AWS Cloud. The architecture consists of Amazon EC2 instances within a VPC behind an Elastic Load Balancer (ELB). A third-party service is used for the DNS. The company's solutions architect must recommend a solution to detect and protect against large-scale DDoS attacks. Which solution meets these requirements?",
    "o": [
      {
        "c": "Enable Amazon GuardDuty on the account.",
        "a": "no"
      },
      {
        "c": "Enable Amazon Inspector on the EC2 instances.",
        "a": "no"
      },
      {
        "c": "Enable AWS Shield and assign Amazon Route 53 to it.",
        "a": "no"
      },
      {
        "c": "Enable AWS Shield Advanced and assign the ELB to it.",
        "a": "yes"
      }
    ],
    "nt": "AWS Shield Advanced provides enhanced DDoS protection for ELB, CloudFront, and Route 53 resources. By assigning the ELB to Shield Advanced, the web application receives always-on detection and automatic mitigations for large-scale DDoS attacks, along with 24/7 DDoS response team support and cost protection for scaling during attacks."
  },
  {
    "id": 1699,
    "q": "A company is building an application in the AWS Cloud. The application will store data in Amazon S3 buckets in two AWS Regions. The company must use an AWS Key Management Service (AWS KMS) customer managed key to encrypt all data that is stored in the S3 buckets. The data in both S3 buckets must be encrypted and decrypted with the same KMS key. The data and the key must be stored in each of the two Regions. Which solution will meet these requirements with the LEAST operational overhead?",
    "o": [
      {
        "c": "Create an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure replication between the S3 buckets.",
        "a": "no"
      },
      {
        "c": "Create a customer managed multi-Region KMS key. Create an S3 bucket in each Region. Configure replication between the S3 buckets. Configure the application to use the KMS key with client-side encryption.",
        "a": "yes"
      },
      {
        "c": "Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with Amazon S3 managed encryption keys (SSE-S3). Configure replication between the S3 buckets.",
        "a": "no"
      },
      {
        "c": "Create a customer managed KMS key and an S3 bucket in each Region. Configure the S3 buckets to use server-side encryption with AWS KMS keys (SSE-KMS). Configure replication between the S3 buckets.",
        "a": "no"
      }
    ],
    "nt": "Multi-Region KMS keys are replicas of the same key material that can be used interchangeably across regions. Using client-side encryption with a multi-Region KMS key ensures the same key encrypts data in both regions, and the data remains accessible even if one region becomes unavailable, with minimal operational overhead for key management."
  }
,{
  "id": 1700,
  "q": "A company recently launched a variety of new workloads on Amazon EC2 instances in its AWS account. The company needs to create a strategy to access and administer the instances remotely and securely. The company needs to implement a repeatable process that works with native AWS services and follows the AWS Well-Architected Framework. Which solution will meet these requirements with the LEAST operational overhead?",
  "o": [
    {
      "c": "Use the EC2 serial console to directly access the terminal interface of each instance for administration.",
      "a": "no"
    },
    {
      "c": "Attach the appropriate IAM role to each existing instance and new instance. Use AWS Systems Manager Session Manager to establish a remote SSH session.",
      "a": "yes"
    },
    {
      "c": "Create an administrative SSH key pair. Load the public key into each EC2 instance. Deploy a bastion host in a public subnet to provide a tunnel for administration of each instance.",
      "a": "no"
    },
    {
      "c": "Establish an AWS Site-to-Site VPN connection. Instruct administrators to use their local on-premises machines to connect directly to the instances by using SSH keys across the VPN tunnel.",
      "a": "no"
    }
  ],
  "nt": "AWS Systems Manager Session Manager provides secure, auditable instance management without the need for bastion hosts, SSH keys, or open inbound ports. By attaching IAM roles with the necessary permissions, administrators can establish secure shell sessions directly through the AWS Management Console or CLI. This approach follows the Well-Architected Framework by eliminating the operational overhead of managing bastion hosts and SSH keys while providing centralized logging and access control through IAM."
}
]